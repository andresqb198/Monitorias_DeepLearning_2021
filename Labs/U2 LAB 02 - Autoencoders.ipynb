{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2.2 - Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://rramosp.github.io/2021.deeplearning/intro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB SUMMARY\n",
    "\n",
    "In this lab we will create a **Sparse Autoencoder**, where we will force the encoder to have **SMALL ACTIVATIONS**. we will continue to use **MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = pd.read_csv(\"local/data/mnist1.5k.csv.gz\", compression=\"gzip\", header=None).values #Se carga la base de datos MNIST local\n",
    "X=(mnist[:,1:785]/255.).astype(np.float32)                  #Se normalizan los datos\n",
    "y=(mnist[:,0]).astype(int)\n",
    "print(\"dimension de las imagenes y las clases\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)      #Se hace un train-test split con 20% para test\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 01: Handcrafted sparse autoencoder\n",
    "\n",
    "\n",
    "Given:\n",
    "\n",
    "- input $X_{in} \\in \\mathbb{R}^{m\\times n}$ = $\\{ x^{(0)}, x^{(1)},..., x^{(n-1)}  \\}$, with $x^{(i)}  \\in \\mathbb{R}^n$\n",
    "\n",
    "- encoder weights and bias: $W_e \\in \\mathbb{R}^{n \\times c}$, $b_e \\in \\mathbb{R}^{c}$\n",
    "- decoder weights and bias: $W_d \\in \\mathbb{R}^{c \\times n}$, $b_e \\in \\mathbb{R}^{n}$\n",
    "\n",
    "with:\n",
    "\n",
    "- $n$ the input data dimension\n",
    "- $m$ the number of input data items\n",
    "- $c$ the autoencoder `code_size`\n",
    "\n",
    "An autoencoder output is computed as a regular neural network\n",
    "\n",
    "$$X_{out} = d(e(X_{in})) = \\sigma(\\text{r}(X_{in} \\times W_e + b_e) \\times W_d  + b_d)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X_{out} \\in \\mathbb{R}^{m\\times n}$ is the output\n",
    "- $d(X) = \\sigma(W_d \\times X + b_d)$ is the decoder function\n",
    "- $e(X) = \\text{r}(X \\times W_e + b_e)$ is the encoder function\n",
    "- $\\sigma(z) = 1/(1+e^{-z})$ is the sigmoid activation function\n",
    "- $r(z) = \\text{max}(0, z)$ is the ReLU activation function\n",
    "\n",
    "and we use the following loss function\n",
    "\n",
    "$$\\text{loss}(X_{in}) = \\frac{1}{m}\\sum_m \\big(x^{(i)} - d(e(x^{(i)}))\\big)^2 + \\beta \\frac{1}{c}\\sum_{c,i} e(x^{(i)})$$\n",
    "\n",
    "observe that we pretend to penalize large values of the encoder activations, with $\\beta$ regulating how much we penalize. As we are using ReLU there is no need to square $e(x^{(i)})$.\n",
    "\n",
    "**Complete the following function to compute the encoder output and loss**. All arguments are `numpy` arrays. The `Xout` output must also be a `numpy` array and `loss` must be a number. You cannot use Tensorflow to implement your function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_autoencoder(Xin, We, be, Wd, bd, beta=0.05):\n",
    "    \n",
    "    sigm = lambda z: 1/(1+np.exp(-z))\n",
    "    relu = lambda z: z*(z>0)\n",
    "    \n",
    "    Xout = ...  #Your code Here\n",
    "    loss = ...  #Your code here\n",
    "    \n",
    "    return Xout, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# --- you should get the following output up to three decimals ---\n",
    "#\n",
    "# Xout\n",
    "#  [[0.53992624 0.54127547 0.40167658 0.59832582]\n",
    "#  [0.580101   0.55012488 0.42321322 0.59509962]\n",
    "#  [0.62155216 0.52174768 0.43006826 0.62407879]\n",
    "#  [0.55635373 0.54059637 0.40857522 0.60072369]\n",
    "#  [0.62178687 0.51694816 0.42812613 0.62813387]]\n",
    "# loss\n",
    "#  0.5723349282191469\n",
    "\n",
    "Xin = np.array([[-0.37035694, -0.34542735,  0.15605706, -0.33053004],\n",
    "                [-0.3153002 , -0.41249585,  0.30073246,  0.13771319],\n",
    "                [-0.30017424, -0.15409659, -0.43102843,  0.38578104],\n",
    "                [-0.14914677, -0.4411987 , -0.33116959, -0.32483895],\n",
    "                [-0.17407847,  0.0946155 , -0.48391975,  0.34075492]])\n",
    "\n",
    "We = np.array([[-0.28030543, -0.46140969, -0.18068483],\n",
    "               [ 0.31530074,  0.29354581, -0.30835241],\n",
    "               [-0.35849794, -0.12389752, -0.01763293],\n",
    "               [ 0.44245022, -0.4465276 , -0.40293482]])\n",
    "\n",
    "be = np.array([ 0.33030961,  0.33221543, -0.32828997])\n",
    "\n",
    "Wd = np.array([[ 0.42964391, -0.22892199,  0.09340045,  0.25372971],\n",
    "               [-0.41209546, -0.23107885, -0.28591832,  0.15998353],\n",
    "               [-0.16731707, -0.10630373, -0.15786946, -0.20899463]])\n",
    "\n",
    "bd = np.array([ 0.32558449,  0.31610265, -0.25844944,  0.28249571])\n",
    "\n",
    "Xout, loss = apply_autoencoder(Xin, We, be, Wd, bd)    #Se realiza la prueba\n",
    "\n",
    "print (\"Xout\\n\", Xout)\n",
    "print (\"loss\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your code with other cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m,c = 4, 5, 3\n",
    "\n",
    "Xin = np.random.random(size=(m,n))-.5\n",
    "We  = np.random.random(size=(n,c))-.5\n",
    "be  = np.random.random(size=c)-.5\n",
    "\n",
    "Wd  = np.random.random(size=(c,n))-.5\n",
    "bd  = np.random.random(size=n)-.5\n",
    "\n",
    "Xout, loss = apply_autoencoder(Xin, We, be, Wd, bd)\n",
    "\n",
    "print (\"Xout\\n\", Xout)\n",
    "print (\"loss\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 02: Sparse autoencoder model\n",
    "\n",
    "Complete the `get_model` function so that the returned model uses the loss function previously defined.\n",
    "\n",
    "You **MUST USE** the **UNSUPERVISED** method to specify your loss function as described in the notes, so that the `.fit` method only gets one argument.\n",
    "\n",
    "\n",
    "Note for models you have to use **Tensorflow operations** in specific you have to use [`tf.reduce_mean`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, code_size, beta=.01):\n",
    "    from tensorflow.keras import Sequential, Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = Input(shape=input_dim, name=\"input\")             #Capa de entrada del autoencoder\n",
    "    encoder = Dense(code_size, activation='relu', name=\"encoder\")(inputs)     #Capa de el encoder\n",
    "    outputs = Dense(input_dim, activation='sigmoid', name=\"decoder\")(encoder)   #Capa de salida del autoencoder\n",
    "\n",
    "    loss = ... #Your code here\n",
    "\n",
    "    model = Model([inputs], [outputs])    #Se crea el modelo con la capa de entrada y la capa de salida\n",
    "    model.add_loss(loss)             #Se agrega la funcion de perdida al modelo\n",
    "\n",
    "    model.compile(optimizer='adam')        #Se configura el modelo con el optimizador adam\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to manually check your code verify that you get the same results with your model and with the function from previous exercise. Observe the possible difference in number precisions (32 vs 64 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and loss from TF model in this task\n",
    "def get_loss(mode, Xinput):\n",
    "    input_layer = model.get_layer(\"input\")           #Se obtiene la capa de entrada del autoencoder\n",
    "    loss_layer = model.get_layer([i.name for i in model.layers if i.name.startswith(\"add_loss\")][0])   #Se optienen las capas de la funcion de perdida\n",
    "    ml = Model(input_layer.input, loss_layer.output)                 #Se crea una instancia de la clase Model con la capa de entrada y la salida de la capa de la funcion de perdida\n",
    "    return (ml(Xinput).numpy())    #Se retorna la salida del modelo con los correspondientes valores de perdida\n",
    "\n",
    "model = get_model(input_dim=X.shape[1], code_size=50, beta=0.05)  #Se crea el modelo\n",
    "print (model(X_train).numpy())  #Se realiza entrenamiento del autoencoding      \n",
    "print (get_loss(model, X_train))  #Se obtiene el resultado de la funcion de perdida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and loss from previous task\n",
    "We, be, Wd, wd = model.get_weights()         #Se obtienen los pesos del modelo\n",
    "Xout, loss = apply_autoencoder(X_train, We, be, Wd, wd, beta=0.05)    #Se obtiene la salida y la perdida del modelo\n",
    "print(Xout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can now train the autoencoder and check its behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, epochs=100, batch_size=32)     #Se entrena el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  test the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = np.random.permutation(X_test)[:10]    #Se realiza una permutacion en X_test y se seleccionan 10 valores\n",
    "X_pred   = model.predict(X_sample)              #Se realiza la prediccion en la muestra correspondiente\n",
    "\n",
    "plt.figure(figsize=(20,5))            #Se dibujan las figuras originales y su respectiva prediccion\n",
    "for i in range(len(X_sample)):\n",
    "    plt.subplot(2,len(X_sample),i+1)\n",
    "    plt.imshow(X_sample[i].reshape(28,28), cmap=plt.cm.Greys_r)\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2,len(X_sample),len(X_sample)+i+1)\n",
    "    plt.imshow(X_pred[i].reshape(28,28), cmap=plt.cm.Greys_r)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion to the corresponding course ntoes, you can inspect the encoder and decoder weights, and also the activations and distributions in the latent space.\n",
    "\n",
    "For activations you should get something similar to this, indicating a much more sparse representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "Image(\"local/imgs/ae_sparse_activations.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
