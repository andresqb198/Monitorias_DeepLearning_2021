{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2.1 - Customized loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://rramosp.github.io/2021.deeplearning/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Fashion MNIST database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()       #Carga la base de datos MNIST\n",
    "X_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])   #Realiza un reshape en los datos de entrenamiento\n",
    "X_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])        #Realiza un Reshape en los datos de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "input_dim = X_train.shape[1]             #Dimensiones de los datos de entrenamiento (Cantidad de caracteristicas)\n",
    "\n",
    "scaler = StandardScaler()  #Preprocesamiento de datos z =(x-u)/s --> u: media: s:desviacion estandar\n",
    "X_trainN = scaler.fit_transform(X_train)    #Transforma los datos de Entrenamiento\n",
    "X_testN = scaler.transform(X_test)          #Transforma los datos de test\n",
    "\n",
    "# convert list of labels to binary class matrix\n",
    "y_trainOHE = utils.to_categorical(y_train)         #Realiza One Hot Encoding en las variables de salida\n",
    "nb_classes = y_trainOHE.shape[1]                   #Cuenta la cantidad de clases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1. Basic model\n",
    "\n",
    "Define a new model using the keras sequential API. The model must have four hidden layers with the following neurons `[128,64,32,16]`. For all the hidden layers use the `relu` activation function. Use `nb_classes` and `softmax` activation for the output layer.\n",
    "\n",
    "You must return an instance of a `Sequential` model. **DO NOT** invoke `compile` or `fit`.\n",
    "\n",
    "Your model structure should be as follows\n",
    "\n",
    "<pre>\n",
    "Model: \"sequential_64\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_308 (Dense)            (None, 128)               100480    \n",
    "_________________________________________________________________\n",
    "dense_309 (Dense)            (None, 64)                8256      \n",
    "_________________________________________________________________\n",
    "dense_310 (Dense)            (None, 32)                2080      \n",
    "_________________________________________________________________\n",
    "dense_311 (Dense)            (None, 16)                528       \n",
    "_________________________________________________________________\n",
    "dense_312 (Dense)            (None, 10)                170       \n",
    "=================================================================\n",
    "Total params: 111,514\n",
    "Trainable params: 111,514\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model(input_dim, nb_classes):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "\n",
    "    model = ... #Your code Here\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_basic_model(input_dim=784, nb_classes=10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_basic_model(input_dim=784, nb_classes=10)   #Se crea el modelo basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers, optimizers\n",
    "\n",
    "# or instantiate an optimizer before passing it to model.compile\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Optimizador del Gradiente descendente estocastico\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)      #Se configura el modelo con la funcion de perdida y el optimizador\n",
    "model.fit(X_trainN[:500,:], y_trainOHE[:500,:], epochs=1000, batch_size=16, validation_split=0, verbose=0)  #Se entrena el modelo\n",
    "\n",
    "preds = model.predict(X_testN, verbose=0)          #Se realizan las predicciones con el conjunto de test\n",
    "print(\"Antes\",preds[0])\n",
    "preds = np.argmax(preds,axis=1)                     #Se obtiene el valor maximo para cada prediccion\n",
    "print(\"Despues\",preds[0])\n",
    "Accuracy = np.mean(preds == y_test)                 #Se mide el accuracy en validacion\n",
    "print('Accuracy = %.2f%s'%(Accuracy*100, '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograma de los pesos de la primera capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.layers[0].weights[0].numpy().flatten(), bins=100);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: $L_2$ regularization\n",
    "\n",
    "Create a model like on TASK 1, but include $L_2$ regularization to every hidden layer (in `kernel_regularizer`) with a regularization parameter equal to 0.0001. \n",
    "\n",
    "Use [tf.keras.regularizer.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2_model(input_dim, nb_classes):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras import regularizers\n",
    "    \n",
    "    model = ... #Your code here\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_L2_model(784, 10)          #Se crea el modelo con el regularizador L2\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect layer regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model2.layers:\n",
    "    print (layer.name, '-->', layer.kernel_regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "model = get_L2_model(784, 10)  #Se crea el modelo con regularizacion L2\n",
    "\n",
    "# or instantiate an optimizer before passing it to model.compile\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)   #Se crea un objeto optimizador SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)           #Se configura el modelo con el optimizador y la funcion de perdida\n",
    "model.fit(X_trainN[:500,:], y_trainOHE[:500,:], epochs=1000, batch_size=16, validation_split=0, verbose=0) #Se entrena el modelo\n",
    "\n",
    "preds = model.predict(X_testN, verbose=0)        #Se realizan las predicciones\n",
    "preds = np.argmax(preds,axis=1)                  #Se obtiene el vector correspondiente a las predicciones\n",
    "Accuracy = np.mean(preds == y_test)              #Se mide el accuracy en el conjunto de validacion\n",
    "print('Accuracy = %.2f%s'%(Accuracy*100, '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograma de los pesos de la primera capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model2.layers[0].weights[0].numpy().flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: $L_1$+$L_2$ regularization\n",
    "\n",
    "Create a model like on TASK 1, but use $L_1$+$L_2$ regularization to every hidden layer (in `kernel_regularizer`) with both regularization parameters equal to 0.0001. \n",
    "\n",
    "Use [tf.keras.regularizer.L1L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L1L2_model(input_dim, nb_classes):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras import regularizers\n",
    "    \n",
    "    model = ... #Your code here\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_L1L2_model(784, 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect layer regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print (layer.name, '-->', layer.kernel_regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "model = get_L1L2_model(784, 10)      #Se crea el modelo con regularizacion L1 y L2\n",
    "\n",
    "# or instantiate an optimizer before passing it to model.compile\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)       #Se crea una instancia del optimizador SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)             #Se configura el modelo con el optimizador y la funcion de perdida\n",
    "model.fit(X_trainN[:500,:], y_trainOHE[:500,:], epochs=1000, batch_size=16, validation_split=0, verbose=0)  #Se entrena el modelo\n",
    "\n",
    "preds = model.predict(X_testN, verbose=0)  #Se realizan las predicciones\n",
    "preds = np.argmax(preds,axis=1)     #Se obtiene el valor correspondiente a la prediccion\n",
    "Accuracy = np.mean(preds == y_test)   #Se calcula el Accuracy en el conjunto de validacion\n",
    "print('Accuracy = %.2f%s'%(Accuracy*100, '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograma de los pesos de la primera capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.layers[0].weights[0].numpy().flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph with the histogram of the network weigths in the first hidden layer. Compare it with the histograms obtained in the previous exercises. What is the effect of applying $L_1$ regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4: Customized loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below to implement the following loss function:\n",
    "\n",
    "$$\\mathcal{L}({\\bf{\\hat{y}}},{\\bf{y}}) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^C {\\bf{1}}_{y_i \\in C_j} w_{j}\\log p_{model}[y_i \\in C_j]$$\n",
    "\n",
    "which corresponds to a weighted version of the categorical cross entropy loss function.\n",
    "\n",
    "Note the following observations:\n",
    "\n",
    "- the function below **returns a function** tied to a specific set of weights. This way we can create different loss functions tied to different weights.\n",
    "- you can assume `y_pred` to be the output of a **softmax** layer. This is, that for each `y` there is a predicted probability for each class $\\in [0,1]$ and summing all up to 1.\n",
    "- before using the `tf.mat.log` function, pass `y_pred` through [`tf.clip_by_value`](https://www.tensorflow.org/api_docs/python/tf/clip_by_value) to ensure any value is between `K.epsilon()` and `1-K.epsilon()`. This is to avoid extreme values close to `0` or close to `1` which might cause numerical issues.\n",
    "- both `y_pred` and `y_true` will be tensors of shape `(m,c)` with `m` being the number of data points and `c` the number of classes.\n",
    "- your answer must be accurate up to three decimal number.\n",
    "- use [`tf.reduce_mean`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean) for the first summation, and [`tf.reduce_sum`](`https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum`) for the second summation with the corresponding `axis` argument.\n",
    "\n",
    "**HINT**: experiment and understand `tf.reduce_mean` and `tf.reduce_sum` before implementing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randint(100, size=(3,5))\n",
    "print (z)\n",
    "print (tf.reduce_mean(z))\n",
    "print (tf.reduce_sum(z, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    from tensorflow.keras import backend as K\n",
    "            \n",
    "    def loss_function(y_true, y_pred):\n",
    "        # clip y_pred to prevent NaN's and Inf's\n",
    "        y_pred =  ...\n",
    "        # compute loss\n",
    "        loss = ...\n",
    "        return loss\n",
    "    \n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually test your code with the following cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([[0.14285714, 0.        , 0.68367347, 0.17346939],\n",
    "                   [0.01020408, 0.60714286, 0.10204082, 0.28061224],\n",
    "                   [0.1733871 , 0.29435484, 0.24193548, 0.29032258],\n",
    "                   [0.25403226, 0.24596774, 0.19758065, 0.30241935],\n",
    "                   [0.52073733, 0.10138249, 0.11981567, 0.25806452],\n",
    "                   [0.47843137, 0.05882353, 0.24313725, 0.21960784]]).astype(np.float32)\n",
    "\n",
    "y_true = np.array([[0, 1, 0, 0],\n",
    "                   [0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1],\n",
    "                   [0, 0, 0, 1],\n",
    "                   [1, 0, 0, 0],\n",
    "                   [1, 0, 0, 0]]).astype(np.float32)\n",
    "\n",
    "\n",
    "loss = weighted_categorical_crossentropy([1,1,1,1])   #Devuelve un metodo para evaluar la perdida con Y_true y Y_pred\n",
    "print (\"loss\", loss(y_true, y_pred).numpy()) # this should return 3.4066\n",
    "loss = weighted_categorical_crossentropy([2,3,4,5])\n",
    "print (\"loss\", loss(y_true, y_pred).numpy()) # this should return 10.7990\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,c = np.random.randint(5)+5,np.random.randint(3)+2\n",
    "y_true = np.eye(c)[np.random.randint(c, size=m)].astype(int)\n",
    "y_pred = np.abs(y_true + np.round(np.random.random(size=(m,c)),2)*2 - .5)\n",
    "y_pred[0,np.argmax(y_true[0])]=0 # force some zero to check clipping\n",
    "y_pred /= np.sum(y_pred,axis=1).reshape(-1,1).astype(np.float32)\n",
    "\n",
    "w = np.round(np.random.random(size=c)*10+1,2)\n",
    "\n",
    "print(\"y_true:\\n\", y_true)\n",
    "print(\"y_pred:\\n\", y_pred)\n",
    "print (\"w:\", w)\n",
    "\n",
    "loss = weighted_categorical_crossentropy(w)\n",
    "\n",
    "print (\"\\nloss: \", loss(y_true, y_pred).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
